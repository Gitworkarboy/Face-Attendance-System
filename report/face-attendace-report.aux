\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{ref:detect-1}
\citation{ref:detect-2,ref:detect-3,ref:detect-4}
\citation{ref:detect-5,ref:detect-6,ref:detect-7}
\citation{ref:detect-8}
\citation{ref:detect-9,ref:detect-10,ref:detect-11}
\citation{ref:detect-7}
\citation{ref:detect-12}
\citation{ref:detect-13}
\citation{ref:detect-14}
\citation{ref:recog-1}
\citation{ref:data-yaleB}
\citation{ref:recog-2}
\citation{ref:recog-3}
\citation{ref:data-lfw}
\citation{ref:recog-4}
\citation{ref:recog-5}
\citation{ref:in-res}
\citation{ref:data-youtube}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\newlabel{introduction}{{I}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {I-A}}Face detection}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {I-B}}Landmark detection}{1}{subsection.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {I-C}}Face recognition}{1}{subsection.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Proposed system}{2}{section.2}}
\newlabel{proposed-system}{{II}{2}{Proposed system}{section.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The system pipeline. There are four stages, namely blur detection, face detection, landmark detection, and face recognition. These four blocks are in the descending order of size in the direction from input to output. This points out that our system is tougher to input frames from the camera when such frames passed through the system. Therefore, best frames are likely to be processed, which may improve the final recognition accuracy.\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:system}{{1}{2}{The system pipeline. There are four stages, namely blur detection, face detection, landmark detection, and face recognition. These four blocks are in the descending order of size in the direction from input to output. This points out that our system is tougher to input frames from the camera when such frames passed through the system. Therefore, best frames are likely to be processed, which may improve the final recognition accuracy.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Implementation}{2}{section.3}}
\newlabel{implementation}{{III}{2}{Implementation}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Motion-blur detection}{2}{subsection.3.1}}
\newlabel{motion-blur-detection}{{\unhbox \voidb@x \hbox {III-A}}{2}{Motion-blur detection}{subsection.3.1}{}}
\newlabel{dft.eq}{{1}{3}{Motion-blur detection}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Face detection}{3}{subsection.3.2}}
\newlabel{face-detection}{{\unhbox \voidb@x \hbox {III-B}}{3}{Face detection}{subsection.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Model structure. Our network consists of a batch input layer and a deep CNN followed by L2 normalization, which results in the face embedding. This is followed by the triplet loss during training.\relax }}{3}{figure.caption.2}}
\newlabel{fig:pipeline}{{2}{3}{Model structure. Our network consists of a batch input layer and a deep CNN followed by L2 normalization, which results in the face embedding. This is followed by the triplet loss during training.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Frontal-view detection}{3}{subsection.3.3}}
\newlabel{frontal-view-detection}{{\unhbox \voidb@x \hbox {III-C}}{3}{Frontal-view detection}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Face recognition}{3}{subsection.3.4}}
\newlabel{face-recognition}{{\unhbox \voidb@x \hbox {III-D}}{3}{Face recognition}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-D}1}Multi-task Convolution Network}{3}{figure.caption.3}}
\citation{ref:in-res}
\citation{ref:facenet}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Pipeline of our cascaded framework that includes three-stage multi-task deep convolutional networks. Firstly, candidate windows are produced through a fast Proposal Network (P-Net). After that, we refine these candidates in the next stage through a Refinement Network (R-Net). In the third stage, The Output Network (O-Net) produces final bounding box and facial landmarks position.\relax }}{4}{figure.caption.3}}
\newlabel{fig:mtcnn}{{3}{4}{Pipeline of our cascaded framework that includes three-stage multi-task deep convolutional networks. Firstly, candidate windows are produced through a fast Proposal Network (P-Net). After that, we refine these candidates in the next stage through a Refinement Network (R-Net). In the third stage, The Output Network (O-Net) produces final bounding box and facial landmarks position.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-D}2}FaceNet model}{4}{subsubsection.3.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The architectures of P-Net, R-Net, and O-Net, where “MP” means max pooling and “Conv” means convolution. The step size in convolution and pooling is 1 and 2, respectively\relax }}{4}{figure.caption.4}}
\newlabel{fig:mtcnn-arch}{{4}{4}{The architectures of P-Net, R-Net, and O-Net, where “MP” means max pooling and “Conv” means convolution. The step size in convolution and pooling is 1 and 2, respectively\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The Triplet Loss minimizes the distance between an anchor and a positive, both of which have the same identity, and maximizes the distance between the anchor and a negative of a different identity.\relax }}{4}{figure.caption.5}}
\newlabel{fig:triplet}{{5}{4}{The Triplet Loss minimizes the distance between an anchor and a positive, both of which have the same identity, and maximizes the distance between the anchor and a negative of a different identity.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\unhbox \voidb@x \hbox {III-D}3}Training}{4}{subsubsection.3.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Schema for Inception-ResNet-v1 and Inception-ResNet-v2 networks. This schema applies to both networks but the underlying components differ.\relax }}{5}{figure.caption.6}}
\newlabel{fig:inception-resnet}{{6}{5}{Schema for Inception-ResNet-v1 and Inception-ResNet-v2 networks. This schema applies to both networks but the underlying components differ.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-E}}Graphic User Interface}{5}{subsection.3.5}}
\newlabel{gui}{{\unhbox \voidb@x \hbox {III-E}}{5}{Graphic User Interface}{subsection.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Parts of GUI Implementation\relax }}{5}{figure.caption.7}}
\newlabel{fig:gui}{{7}{5}{Parts of GUI Implementation\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces GUI layout\relax }}{5}{figure.caption.8}}
\newlabel{fig:user_guide}{{8}{5}{GUI layout\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-F}}Attendance management}{5}{subsection.3.6}}
\newlabel{attendance-management}{{\unhbox \voidb@x \hbox {III-F}}{5}{Attendance management}{subsection.3.6}{}}
\citation{ref:t-sne}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces User Guide\relax }}{6}{table.caption.9}}
\newlabel{tab:user_guide}{{I}{6}{User Guide\relax }{table.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces New standard excel form\relax }}{6}{figure.caption.10}}
\newlabel{fig:form-new}{{9}{6}{New standard excel form\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experimental result}{6}{section.4}}
\newlabel{experimental-result}{{IV}{6}{Experimental result}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Excel form contain pre-inputed data\relax }}{6}{figure.caption.11}}
\newlabel{fig:form-data}{{10}{6}{Excel form contain pre-inputed data\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Form is under checking\relax }}{6}{figure.caption.12}}
\newlabel{fig:form-checked}{{11}{6}{Form is under checking\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Embeddings}{6}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Training}{6}{subsection.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion}{6}{section.5}}
\newlabel{conclusion}{{V}{6}{Conclusion}{section.5}{}}
\bibcite{ref:detect-1}{1}
\bibcite{ref:detect-2}{2}
\bibcite{ref:detect-3}{3}
\bibcite{ref:detect-4}{4}
\bibcite{ref:detect-5}{5}
\bibcite{ref:detect-6}{6}
\bibcite{ref:detect-7}{7}
\bibcite{ref:detect-8}{8}
\bibcite{ref:detect-9}{9}
\bibcite{ref:detect-10}{10}
\bibcite{ref:detect-11}{11}
\bibcite{ref:detect-12}{12}
\bibcite{ref:detect-13}{13}
\bibcite{ref:detect-14}{14}
\bibcite{ref:recog-1}{15}
\bibcite{ref:recog-2}{16}
\newlabel{fig:pca}{{12a}{7}{Visualization using PCA.\relax }{figure.caption.13}{}}
\newlabel{sub@fig:pca}{{a}{7}{Visualization using PCA.\relax }{figure.caption.13}{}}
\newlabel{fig:t-sne}{{12b}{7}{Visualization using t-SNE.\relax }{figure.caption.13}{}}
\newlabel{sub@fig:t-sne}{{b}{7}{Visualization using t-SNE.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces 128-dimensional embeddings of the training set are projected to the 3D space using PCA (a) and t-SNE (b) techniques. In general, although samples of different classes are not totally separated, the PCA method partly reveals the separation of classes. Meanwhile, the t-SNE method provides a better view of classes as distinguished clusters. Best viewed in color.\relax }}{7}{figure.caption.13}}
\newlabel{fig:visualization}{{12}{7}{128-dimensional embeddings of the training set are projected to the 3D space using PCA (a) and t-SNE (b) techniques. In general, although samples of different classes are not totally separated, the PCA method partly reveals the separation of classes. Meanwhile, the t-SNE method provides a better view of classes as distinguished clusters. Best viewed in color.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Unrecognized identity, (a),(b) are training images, and (c),(d) are testing images in pratical condition.\relax }}{7}{figure.caption.14}}
\newlabel{fig:compare}{{13}{7}{Unrecognized identity, (a),(b) are training images, and (c),(d) are testing images in pratical condition.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{References}{7}{section*.16}}
\bibcite{ref:recog-3}{17}
\bibcite{ref:recog-4}{18}
\bibcite{ref:recog-5}{19}
\bibcite{ref:in-res}{20}
\bibcite{ref:facenet}{21}
\bibcite{ref:data-yaleB}{22}
\bibcite{ref:data-lfw}{23}
\bibcite{ref:data-youtube}{24}
\bibcite{ref:t-sne}{25}
