%------------------------------------------------------------------------------
%	Setup
%------------------------------------------------------------------------------
\documentclass[journal, twocolumn]{IEEEtran}


\usepackage{graphicx}
\usepackage[breaklinks=true]{hyperref}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amssymb}

\usepackage{caption}
\usepackage{subcaption}
\captionsetup{font=footnotesize}

\usepackage{textcase}
\usepackage[tablename=TABLE]{caption}
\DeclareCaptionTextFormat{up}{\MakeTextUppercase{#1}}
\captionsetup[table]{
    labelsep=period,
    justification=centering,
    textformat=up,
}


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%------------------------------------------------------------------------------
%	Title
%------------------------------------------------------------------------------
\title{An Automatic Face Attendance Checking System using Deep Facial Recognition Technique}


%------------------------------------------------------------------------------
%	Author
%------------------------------------------------------------------------------
\author{Thuy Nguyen-Chinh,
		Thien Do-Tieu,
		Phuong Le-Van-Hoang,
		Sy Nguyen-Tan,
		Qui Nguyen-Van,
		Phu Nguyen-Tan

\thanks{This work is our assignment in the course of "Artificial Intelligence in Control Engineering" Sep-Dec 2018, guided by Dr. Pham Viet Cuong (email: pvcuong@hcmut.edu.vn), Faculty of Electrical and Electronics Engineering, HoChiMinh city University of Technology.}
\thanks{Authors are senior of the Faculty of Electrical and Electronics Engineering, HoChiMinh city University of Technology (e-mail: \{thuy.ng.ch, dotieuthien9997, hpcqt97, tansyab1, nvqui97, tanphu97.nguyen \}@gmail.com).}
\thanks{The software is open source and can be found in \url{https://github.com/AntiAegis/Face-Attendance-System}.}
}


\maketitle


%------------------------------------------------------------------------------
%	Abstract
%------------------------------------------------------------------------------
\begin{abstract}
Nowadays, as computers are powerful enough for implementing complex algorithms, there are numerous applications that people utilize computers to run. In which, facial recognition is one of the most active fields of applications. In fact, computers can not only automatically identify who a person is, but also operate 24/7, which human beings cannot endure. This leads to the replacement of people by computers in some repetitive and real-time applications.

In this work, we apply the facial recognition into an attendance checking system that uses faces of registered people to check their attendance. This system has a GUI which allows easy user-to-system interaction. The core of the system is a deep facial recognition technique, which has four stages (e.g., removing motion-blur frames, detecting faces, removing non-frontal-view faces, and recognizing). Particularly, in the recognition phase, we consider this stage as an open-set facial recognition problem, so the system is able to detect people who have not registered in the database before. Also, we boost the performance of the system by utilizing hardware resources of users' computers. Although the system is designed to run with a low-resolution webcam, its performance is reasonably accurate on our private dataset.
\end{abstract}


\begin{IEEEkeywords}
Face Attendance Checking, Facial Recognition, Deep Learning
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle


%------------------------------------------------------------------------------
%	Introduction
%------------------------------------------------------------------------------
\section{Introduction}
\label{introduction}
%\textbf{This section is of Thien.}
%Introduce about a framework of face recognition system, including face detection, landmark detection, face recognition.

Face recognition systems are applying widely in real life, such as: tracking, managing employees, finding information of ceblerities, etc. There are many approaches to design a face recognition system, but these systems frequently are affected by light, non-frontal faces, resolution of cameras, etc, each method have many separable challenges. Overall, a face recognition has two main stages which are face detection and face recognition, yet we want to create a constraint on frontal faces for users, that lead our system to have three stages: face detection, face landmark detection and face recognition.  
\subsection{Face detection}
Face detection and alignment are essential to many face applications such as face recognition and facial expression analysis. However, the large visual variations of faces, such as occlusions, large pose variations and extreme lightings, impose
great challenges for these tasks in real world applications.

The cascade face detector proposed by Viola and Jones \cite{ref:detect-1} utilizes Haar-Like features and AdaBoost to train cascaded classifiers, which achieve good performance with real-time
efficiency. However, quite a few works [2, 3, 4] indicate that this detector may degrade significantly in real-world applications with larger visual variations of human faces even with more advanced features. Besides the cascade structure, [5, 6, 7] introduce deformable part models (DPM) for face detection and achieve remarkable performance. However, they need high computational expense and may usually require expensive annotation in the training stage. Recently, convolutional neural networks (CNNs) achieve remarkable progresses in a variety of computer vision tasks, especially face detection task. Li et al. [19] use cascaded CNNs for face detection, but it requires bounding box calibration from face detection with extra computational expense and ignores the inherent correlation between facial landmarks localization and bounding box regression. Face alignment also attracts extensive interests. Regression-based methods [12, 13, 16] and template fitting approaches [14, 15, 7] are two popular categories.

However, most of the available face detection and face alignment methods ignore the correlation between these two tasks. Though there exist several works attempt to jointly solve them, there are still limitations in these works. For example, Chen et al. [18] jointly show alignment and detection with random forest using features of pixel value difference. But, the handcraft features used limits its performance. From those previous experiments, we choose an new approach which integrate these two tasks using unified cascaded CNNs by multi-task learning called Multi-task Convolutional Network in section \ref{face-recognition}.
\subsection{Landmark detection}
The locations of the fiducial facial landmark points around facial components and facial contour capture  the  rigid  and  non-rigid  facial  deformations  due to  head  movements  and  facial  expressions.  They  are hence important for various facial analysis tasks. Many facial landmark detection algorithms have been developed to automatically detect those key points over the years. In this paper, we use dlib library which is a powerfull source for face and facial landmark detection. We will discuss our implement in detail in section \ref{frontal-view-detection}.

\subsection{Face recognition}
After face detection and alignment, those regions of face is extracted to get feature vectors. With conventional way, One of the most popular feature for face recognition is Gabor feature. Tudor Barbu ?? uses Gabor transfrom to extract feature, and then using K-Nearest Neighbour (K-NN) based on clustering feature to predict identity of a face. This implement achieve quite impressed performance with accuracy of 90\% on Yale Face Database B. In Opencv library which focuses on algorithms of Computer Vision introduces a method called Local Binary Patterns (LBP) based on Haar-Like feature. In term of speed, LBP has relly real-time efficiency, whereas it is not stable in term of accuracy, this method cannot face with arounded noise which is the reason why LBP and Haar-Like feature are rarely applied in practical systems. Because of limitations of conventional features,  deep learning models gradually instead and get better and better. Yi Sun, Xiaogang Wang, Xiaoou Tang ?? build a deep model Deep hidden IDentity features (DeepID) which uses convolutional neural network to extract face feature. Advantage of this model is using a small dataset for training, that is consideraby good for systems which cannot collect a large dataset of users. However, to reach a high accuracy, DeepID model become really complex with many neural network branches for each person. There are 10 patches of face which contain interested information are chosen from each image, then they are scaled with three figures in RGB and gray chanel. Totally, model have 60 different networks to extact feature of an image, then feedforwarding feature into a classifier using Joint Bayesian. DeepID achieves excellent accuracy of 97.45\% on dataset Labeled Faces in the
Wild (LFW). In 2014, the authors of DeepID show DeepID2 which is a improved version of DeepID. In new version, interested regions of face algorithm is built to eliminate useless patches which cannot extract high-level feature. That work really helpfull affect accuracy, specifically there is a increase in accuracy at 99.63\%. In 2015, Google Inc.?? use deep convolutional network Inception and triplet loss function in FaceNet mdoel to extract feature. Their outstanding work in this model is using hard triplet loss to separate feature for each person, so FaceNet feature is robust in both face verification and face recognition. The accuracy of 99.63\% on LFW and 95.12\% on Youtube Faces
DB dataset is high enough to represent the perfection of model.
%------------------------------------------------------------------------------
%	Proposed system
%------------------------------------------------------------------------------
\medskip
\section{Proposed system}
\label{proposed-system}

In this paper, we apply deep facial recognition techniques into the problem of face attendance checking. A system is built in order to manage appearances of students in a class, which is revealed in Fig. \ref{fig:system}. As normally, a facial recognition system is organized as a pipeline of typical stages such as face detection, landmark detection, and face recognition. However, to ensure input frames for underlying algorithms are high quality, we append an early filter (the Blur detection stage) that are able to discard blur frames, which are caught by motions of people in front of a standard webcam. Then, clean frames are passed through the Face detection block to count the number of faces existing inside these frames. In our specific case, there is only one face per frame allowed to be processed, so frames that contain more than one faces are rejected by the block. Afterward, the Landmark detection is to localize statistic-salient points in the face in order to verify whether the face is in frontal view of the camera. This frontal-view check will help improve the accuracy of the face recognition algorithm. Finally, the Face recognition uses blur-clean, one-face, and frontal-view frames from previous stages to extract relevant features and then perform a classification task to indicate which category the input most likely belong to.

In addition, to leverage the ease in use, we design a friendly graphic user interface (GUI) so that people who want to use the system to manage (teachers) or check (students) attendance can interact with the application without any specific knowledge. To make the system more robust, we carefully analyze the distribution outlier of features representing for registered accounts. Therefore, the algorithm has ability to detect people who have not registered in the application before, which is equivalent to the open-set problem in face recognition.

Our work is organized as follows. In the section \ref{implementation}, stages of the proposed system are described clearly, including motion-blur detection, face detection, frontal-view detection, and face recognition. Then, section \ref{experimental-result} is for reporting some experimental results.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{img/system-pipeline.png}
	\caption{The system pipeline. There are four stages, namely blur detection, face detection, landmark detection, and face recognition. These four blocks are in the descending order of size in the direction from input to output. This points out that our system are tougher to input frames from the camera when such frames passed through the system. Therefore, best frames are likely to be processed, which may improves the final recognition accuracy.}\label{fig:system}
\end{figure}


%------------------------------------------------------------------------------
%	Implementation
%------------------------------------------------------------------------------
\medskip
\section{Implementation}
\label{implementation}

\subsection{Motion-blur detection}
\label{motion-blur-detection}
\textbf{This section is of Phu.}

The first stage of this system is detecting blurred image and rejecting them out of next stage. We know that the blurred image means each pixel in the source image gets spread out and mixed into surrounding neighbour pixels. For our attendance checking system, the motion blur happens when an object (namely face or webcam) moves during the exposure. So as to detect whether an image is blurred, we use the 2D-FFT (2D-Fast Fourier Transform) method.

 We will review about Fourier Transform of Images. To compute the Fourier transform of an image, you need to:
\begin{itemize}
\item Compute DFT of each row, in place.
\item Compute DFT of each column, in place.
\end{itemize}  

When a signal is discrete and periodic, we use the discrete Fourier transform, or DFT.
Suppose our signal is $a_n$ for $n=0 \ldots N-1$,
and $a_n=a_{n+jN}$ for all $n$ and $j$.
The spectrum of $a$ is:
\begin{equation}
A_k = \sum_{n=0}^{N-1} W_N^{kn} a_n
\label{dft.eq}
\end{equation}
where
$$
W_N = e^{-i {2 \pi \over N}}
$$
and $W_N^k$ for $k=0 \ldots N-1$ are called the {\it Nth roots of unity}.
The sequence $A_k$ is the discrete Fourier transform of the sequence $a_n$.
Each is a sequence of $N$ complex numbers.

The FFT is a fast algorithm for computing the DFT.
If we take the 2-point DFT and 4-point DFT and generalize them
to 8-point, 16-point, ..., $2^n$-point (n is an integer), we get the FFT algorithm.

There are several ways to write an FFT. For instance, let $m$ be an integer and let $N=2^m$.
Suppose that $x=[x_0,\ldots,x_{N-1}]$ is an $N$ dimensional complex vector.
Let $\omega=\exp({-2\pi i\over N})$.  Then the DFT, $c= F_N(x)$ is given by
\begin{equation}
c_k={1\over N}\sum_{j=0}^{j=N-1}x_j\omega^{jk}.
\end{equation}
Let $n=N/2$, let $u$ and $v$ be $n$ dimensional vectors defined by 
\begin{align}
u_j&=x_j+x_{j+n},\ j=0,\ldots,n-1\\
v_j&=(x_j-x_{j+n})\omega ^j,\ j=0,\ldots,n-1.
\end{align}
Then 
\begin{align}
c_{2j}={1\over 2}(F_n(u))_j,\ j=0,\ldots,n-1\\
c_{2j+1}={1\over 2}(F_n(v))_j,\ j=0,\ldots,n-1.
\end{align}

To compute the DFT of an $N$-point sequence using equation (\ref{dft.eq})
would take $(N^2)$ multiplies and adds.
The FFT algorithm computes the DFT using $(N \log N)$ multiplies and adds.

Practical issues:
We translate the picture so that pixel (0,0), which now contains frequency $(\omega_x,\omega_y)=(0,0)$, moves to the center of the image. Then, we display pixel values proportional to
log(magnitude) of each complex number. For color images, do the above to each of the three channels
(R, G, and B) independently.

Apply to our system, firstly, we calculate FFT of image. Secondly, we will compute mean amplitude spectrum value of entire pixel in image and. Finally, the result of this operation is compared to an optimal threshold which distinguishes blurred and non-blurred image as accurate as possible. The image is called non-blurred if and only if its average value greater than the threshold value, and vice versa. After that, non-blurred images are applied to face detection stage of system.

\subsection{Face detection}
\label{face-detection}
In this paper, we have used Histogram of Oriented Gradients method for extracting features of the face and Linear Support Vector Machine (SVM) method for face detections.\\
The implementation of this method using sliding window technique with the different sizes of the windows. Using the sliding window technique we could complete the calculation of HOG features, applied to detect and differentiate the face and the false face recognition using the SVM technique.\\
All of the pre-processing steps are automatically implemented before using Dlib library with the input of being given facial images and the output of localization the identified faces.\\

\subsection{Frontal-view detection}
\label{frontal-view-detection}
To check whether the shape of the faces has to be frontal, we implement these 3 following steps:
\begin{enumerate}[Step 1.]
	\item Focusing on the center of the image. Only accept these faces that locate in the most central of the images. $((120-360), (90,360))$
	\item Identify the skew of the image: calculate the coordinate of these eyes and the angle deviation between two eyes in the horizontal direction. If the angle deviation is larger than 10 degree, the image will be ignored.
	\item Identify the rotation of the image: choose the point which is the midpoint of the right and the left eye. If the nose which is deviated from the selected point is greater than 10 pixels in the horizontal direction, the image is ignored.
\end{enumerate}
These steps are implemented based on 5-point facial landmark technique with Dlib library instead of 68-point facial landmark in order to improve performance. If the image satisfies the condition, it will be accepted.


\subsection{Face recognition}
\label{face-recognition}
%\textbf{This section is of Thien.}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/pipeline.png}
	\caption{Model structure. Our network consists of a batch input layer and a deep CNN 
followed by L2 normalization, which results in the face embedding. This is followed by the triplet loss during training.}\label{fig:pipeline}
\end{figure}

In this stage, faces in raw images are detected and aligned by Multi-task CNN, we use convenient pre-trained FaceNet model to extract feature (in Figure \ref{fig:pipeline}) and then feedforward it to a SVM classifier for regconition. 
\subsubsection{Multi-task Convolution Network}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/mtcnn.png}
	\caption{Pipeline of our cascaded framework that includes three-stage multi-task deep convolutional networks. Firstly, candidate windows are produced
through a fast Proposal Network (P-Net). After that, we refine these candidates
in the next stage through a Refinement Network (R-Net). In the third stage,
The Output Network (O-Net) produces final bounding box and facial landmarks position.}\label{fig:mtcnn}
\end{figure}

The overall pipeline Multi-task CNN is shown in Figure \ref{fig:mtcnn}. An image is  initially resized to different scales to build an image pyramid, which is the input of the following three-stage cascaded framework with CNN architectures in Figure \ref{fig:mtcnn-arch}:

\textbf{Stage 1}: A fully convolutional network is exploited, called Proposal Network (P-Net), to obtain the candidate windows and their bounding box regression vectors. Then using the estimated bounding box regression vectors to calibrate the candidates. After that, employing non-maximum suppression (NMS) to merge highly overlapped candidates.

For each candidate window, P-CNN predict the offset between it and the nearest ground truth (i.e.,the bounding boxes’ left top, height, and width). The learning objective is formulated as a regression problem, and  the Euclidean loss is employed for each sample $x_i$:
\begin{align}
	L_i^{box} = \Vert y_i^{prediction} - y_i^{truth} \Vert _2^2
\end{align}
		
\textbf{Stage 2}: All candidates are fed to another CNN, called Refine Network (R-Net), which further rejects a large number of false candidates, performs calibration with bounding box regression, and NMS candidate merge.

\textbf{Stage 3}: his stage is similar to the second stage, but in this stage we aim to describe the face in more details. In particular, the network will output five facial landmarks’ positions.

Similar to the bounding box regression task, facial landmark detection is formulated as a regression problem: 

\begin{align}
	L_i^{landmark} = \Vert y_i^{prediction} - y_i^{truth} \Vert _2^2
\end{align}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/mtcnn_arch.png}
	\caption{The architectures of P-Net, R-Net, and O-Net, where “MP” means max pooling and “Conv” means convolution. The step size in convolution and pooling
is 1 and 2, respectively}\label{fig:mtcnn-arch}
\end{figure}

\subsubsection{FaceNet model} 
This model use Inception-ResNet v1 architecture ?? and triplet loss to extract feature. Inception-ResNet v1 (in Figure \ref{fig:inception-resnet}) is a very deep convolutional network which combine ResNet network and Inception network with a complex structure. This deep network affords to extract high-level feature for object recognition, and combination with triplet loss gets better and better. 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/triplet.png}
	\caption{The Triplet Loss minimizes the distance between an anchor and a positive, both of which have the same identity, and maximizes the distance between the anchor and a negative of a different identity.}
	\label{fig:triplet}
\end{figure}

The embedding is represented by $f(x) \in R^d$. It embeds an image $x$ into a $d$-dimensional Euclidean space. Here we want to ensure that an image $x_i^a$ (anchor) of a specific person is closer to all other images $x_i^p$ (positive) of the same person than it is to any image $x_i^n$ (negative) of any other person. This is visualized in Figure \ref{fig:triplet}. Thus we want, 
\begin{align}
	\Vert f(x_i^a) - f(x_i^p) \Vert_2^2 + \alpha < \Vert f(x_i^a) - f(x_i^n) \Vert_2^2
\end{align}
where $\alpha$ is a margin that is enforced between positive and negative pairs. The loss that is being minimized is then: 
\begin{align}
	L = \sum_i^N(\Vert f(x_i^a) - f(x_i^p) \Vert_2^2  - \Vert f(x_i^a) - f(x_i^n) \Vert_2^2 + \alpha)
\end{align}
In this implement, FaceNet model is trained on VGGFace2 dataset which over 9000 identities and over 3.3 million faces. VGGFace2 is a large-scale face recognition dataset from Google Image Search and have large variations in pose, age, illumination, ethnicity and profession. Therefore, embeddings is specifical for each person. 
\subsubsection{Training} 
To apply Multi-task model and FaceNet model into Face Attandence system, we feedforward raw images of students into Multi-task CNN to get face patches, then extract feature of each patch with 512 dimension vector by pre-trained FaceNet model. After that, we split dataset of students into 3 subsets: training, validating and testing. Each person in training, validating, testing subset contrains 30 images, 10 images and 30 images respectively. We decide to collect only 30 images for training subset, because we want to reduce time of training and time for collecting images. Next, we use SVM classifier to train on trainning subset and validate on validating subset. The output of trained-classifier is probability vector whose each element represent ability of an identity anchor belong to. Thus, anchor is determined by choose which identity have maximum probability, that suffer from mistakes when faces of strangers appear. To solve open-set problem, we combine both two subsets: training and validating to training threshold. This threshold helps us to eliminate unknow people, additionally ensure that the result of system achieve higher accuracy.
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{img/in_res_v1.png}
	\caption{Schema for Inception-ResNet-v1 and Inception-ResNet-v2 networks. This schema applies to both networks but the underlying components differ.}
	\label{fig:inception-resnet}
\end{figure}

\subsection{Graphic User Interface}
\label{gui}
\textbf{This section is of Sy.}


%----------------------------Attendance management-----------------------------
\subsection{Attendance management}
\label{attendance-management}
\textbf{This section is of Phuong.}

This is the final phase of Face Attendance Checking System.
It was designated to mark the presence of one resulted from
our algorithm in a file of excel format, namely xlsx extension.
To be used by the system, the excel file must meet
a stringent format made up of essential contents and be
generated by the GUI. 

Fig. \ref{fig:form-new} depicts a new standard empty
excel table generated by our GUI. After obtaining a new file, we should fill in the table with the desired data (Fig. \ref{fig:form-data}). The most special things in this table are column ID and Total. 
ID is considered a primary key because the algorithm will mark the presence of a specific person via his ID. 
To help the host in easy attendance management, we designed the column Total with a view to showing the number of absences in all.

Fig. \ref{fig:form-checked} depicts an excel file's content after a checking progress finished. 
The GUI will automatically insert the only one new day column between Group and Total ones and in the tail of previous checked day. 
Letter 1 will be marked as presence in a cell of this column accordant to an ID. 
After attendance checking process is completed, the Total column will display the number of absences of previous days and the current one. Smartly can it display as we specially assigned a size-dynamic sum function to each cell of this column.


\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{img/form-new.png}
	\caption{New standard excel form}\label{fig:form-new}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{img/form-data.png}
	\caption{Excel form contain pre-inputed data}\label{fig:form-data}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{img/form-checked.png}
	\caption{Form is under checking}\label{fig:form-checked}
\end{figure}


%------------------------------------------------------------------------------
%	Result
%------------------------------------------------------------------------------
\medskip
\section{Experimental result}
In this section, we first evaluate the effectiveness of the feature extracted from FaceNet. Then we will compare our system in different context such as: background, illumination, resolution of camera.Finally, we evaluate the computational efficiency of our system.
\label{experimental-result}
%\textbf{This section is of Thien.}
\subsection{Embedings} 
\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=\linewidth]{img/pca1.png}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=\linewidth]{img/pca2.png}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\linewidth}
    \includegraphics[width=\linewidth]{img/pca3.png}
    \caption{}
  \end{subfigure}
  \caption{Embeddings with PCA visualization} 
  \label{fig:pca}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{img/t-sne.png}
	\caption{Embeddings with t-SNE visualization}\label{fig:t-sne}
\end{figure}
Because we use pre-trained model FaceNet, we need to test specification of embeddings which is output of model. We use PCA ?? and t-SNE ?? to visualize embeddings in 3 dimension space in Figure \ref{fig:pca} and Figure \ref{fig:t-sne}. In PCA visualization, embeddings of the same person close together, although they are not completely separable. In another of t-SNE, because t-SNE method include clustering stage, so embeddings totally belong to their classes. If embeddings of FaceNet model is not contain high-level of specification, reducing dimension algorithms cannot show or cluster embeddings properly.  
\subsection{Training}
Training data is carefully collected with different views from $-70^\circ$ to $70^\circ$. This work can improve accuracy in pratical system ,because it is difficult for users to keep their faces in a correct position. In training data include 52 identities and 1560 images totally.

The accuracy of SVM classifier is 99.36\%, after training classifier, we train to get the best threshold. We divide threshold in range $[0;1]$.As a result, threshold for 52 identities is 0.18825. Testing accuracy with threshold achieve 98.85\% on testing subset. In practical environment, we test on 32 identities, three are 29 identites recognized easily and 3 identities who are not recognized continuously. In Figure \ref{fig:compare}, the (a),(b) are training images and (c),(b) are testing images, the effect of different ilumination lead the probabilities of testing anchor are lower than threshold, so training data have to cover many real-life cases to create the best classifier.  

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.35\linewidth}
    \includegraphics[width=\linewidth]{img/1.png}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.35\linewidth}
    \includegraphics[width=\linewidth]{img/2.png}
    \caption{}
  \end{subfigure}
  \begin{subfigure}[b]{0.35\linewidth}
    \includegraphics[width=\linewidth]{img/3.png}
    \caption{}
  \end{subfigure}
    \begin{subfigure}[b]{0.35\linewidth}
    \includegraphics[width=\linewidth]{img/4.png}
    \caption{}
  \end{subfigure}
  \caption{Unrecognized identity, (a),(b) are training images, and (c),(d) are testing images in pratical condition.} 
  \label{fig:compare}
\end{figure}


%------------------------------------------------------------------------------
%	Conclusion
%------------------------------------------------------------------------------
\medskip
\section{Conclusion}
\label{conclusion}

In this work, we applied the deep facial recognition techniques to solve the problem of face attendance checking. The system has a pipeline with four stages (e.g., motion-blur detection, face detection, landmark detection, and face recognition). Besides, the system is also integrated a friendly GUI, which allows users both teachers and students interact with it in an easy way. On our private dataset, the application perform accurate despite of the low-resolution webcam of typical laptops. This demonstrates that our underlying algorithm is effective to deal with this poor-quality input problem.

In the future, we will target to widen our dataset so that the dataset will be asymptotic to real applications. In addition, more algorithms will be considered to improve the ability of the algorithm to discriminate feature distributions of output classes.


%------------------------------------------------------------------------------
%	Acknowledgment
%------------------------------------------------------------------------------
\section*{Acknowledgment}

The authors would like to thank Dr. Pham Viet Cuong for providing documents as well as chance for us to do this work. Also, the authors would like to thank ...


%------------------------------------------------------------------------------
% References
%------------------------------------------------------------------------------
\begin{thebibliography}{9}
\input{bibliography.tex}
\end{thebibliography}

\end{document}
